{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EDIT VARIABLES HERE\n",
    "# pretrained_model_name_or_path = '/tmp/gte-Qwen1.5-7B-instruct' # on the other gpu\n",
    "pretrained_model_name_or_path = '/storage/huggingface_model/gte-Qwen1.5-7B-instruct' # on bonita\n",
    "\n",
    "dataset = \"counseling\"\n",
    "if dataset == \"original\":\n",
    "    save_embed_dir = 'save/embeddings/'\n",
    "    save_kl_dir = 'save/kl_divergence/all_tokens/' \n",
    "    train_name = 'HC3_en_train'\n",
    "    val_name = 'HC3_en_valid'\n",
    "    train_path = 'dataset/processed_data/train_valid_data/HC3_en_train.json'\n",
    "    val_path = 'dataset/processed_data/train_valid_data/HC3_en_valid.json'\n",
    "    dataset_names = ['pub', 'writing', 'xsum']\n",
    "    labels_folder = 'labels/'\n",
    "elif dataset == \"counseling\":\n",
    "    save_embed_dir = 'save/counseling_embeddings/'\n",
    "    save_kl_dir = 'save/counseling_kl_divergence/all_tokens/' \n",
    "    train_name = 'train'\n",
    "    val_name = 'val'\n",
    "    train_path = 'dataset/processed_counseling/train.json'\n",
    "    val_path = 'dataset/processed_counseling/val.json'\n",
    "    test_path = 'dataset/processed_counseling/test.json'\n",
    "    labels_folder = 'dataset/processed_counseling/'\n",
    "    dataset_names = ['counseling']\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.makedirs(save_embed_dir, exist_ok=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/iws/clarisw/miniconda3/envs/jupenv/lib/python3.11/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 939531264 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfe4263bc9e4f27a0e8526fdc08660f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model.embed_tokens': 0, 'model.layers.0': 1, 'model.layers.1': 1, 'model.layers.2': 2, 'model.layers.3': 2, 'model.layers.4': 'cpu', 'model.layers.5': 'cpu', 'model.layers.6': 'cpu', 'model.layers.7': 'cpu', 'model.layers.8': 'cpu', 'model.layers.9': 'cpu', 'model.layers.10': 'cpu', 'model.layers.11': 'cpu', 'model.layers.12': 'cpu', 'model.layers.13': 'cpu', 'model.layers.14': 'cpu', 'model.layers.15': 'cpu', 'model.layers.16': 'cpu', 'model.layers.17': 'cpu', 'model.layers.18': 'cpu', 'model.layers.19': 'cpu', 'model.layers.20': 'cpu', 'model.layers.21': 'cpu', 'model.layers.22': 'cpu', 'model.layers.23': 'cpu', 'model.layers.24': 'cpu', 'model.layers.25': 'cpu', 'model.layers.26': 'cpu', 'model.layers.27': 'cpu', 'model.layers.28': 'cpu', 'model.layers.29': 'cpu', 'model.layers.30': 'cpu', 'model.layers.31': 'cpu', 'model.norm': 'cpu', 'lm_head': 'cpu'}\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "embedding_dim = 4096\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, trust_remote_code=True, local_files_only=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, trust_remote_code=True,device_map='auto', local_files_only=True)\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(path, file_name, save_kl):\n",
    "    \n",
    "    def last_token_pool(last_hidden_states, attention_mask):\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "            return last_hidden_states[torch.arange(batch_size, device='cpu'), sequence_lengths]\n",
    "    \n",
    "    \n",
    "    def get_kl_and_embeddings(input_texts, save_kl):\n",
    "        # print('get_kl_and_embeddings being called')\n",
    "        num_tokens = tokenizer(input_texts, return_tensors='pt', truncation=False, padding=False)['input_ids'].shape[1]\n",
    "\n",
    "        max_length = 300\n",
    "        batch_dict = tokenizer(input_texts, max_length=max_length, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch_dict,output_hidden_states=True)\n",
    "            if save_kl: \n",
    "                # todo: used for test\n",
    "                last_logits = model.lm_head(outputs.hidden_states[-1]).squeeze()\n",
    "                first_logits = model.lm_head(outputs.hidden_states[0]).squeeze()\n",
    "                \n",
    "        \n",
    "        all_embed = [last_token_pool(outputs.hidden_states[i].cpu(), batch_dict['attention_mask']) for i in range(len(outputs.hidden_states))]\n",
    "        all_embed_concated = torch.concat(all_embed,1).cpu()\n",
    "\n",
    "        if save_kl: \n",
    "            # # todo: used for test\n",
    "            # last_logits = all_embed[-1]\n",
    "            # # first_logits = all_embed[0]\n",
    "            # first_logits = all_embed[1]\n",
    "        \n",
    "            kls = []\n",
    "            for i in range(1,len(outputs.hidden_states)-1):\n",
    "                with torch.no_grad():\n",
    "                    middle_logits = model.lm_head(outputs.hidden_states[i]).squeeze()\n",
    "                    # todo: used for test\n",
    "                    # middle_logits = all_embed[i]\n",
    "                kls.append(F.kl_div(F.log_softmax(middle_logits, dim=-1), F.softmax(first_logits, dim=-1), reduction='batchmean').item()+\n",
    "                        F.kl_div(F.log_softmax(middle_logits, dim=-1), F.softmax(last_logits, dim=-1), reduction='batchmean').item())\n",
    "            return kls, all_embed_concated\n",
    "        \n",
    "        return all_embed_concated\n",
    "\n",
    "\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    if file_name == 'HC3_en_train':\n",
    "        data = data[:200]\n",
    "    elif file_name == 'HC3_en_valid':\n",
    "        data = data[:40]\n",
    "\n",
    "    kls = []\n",
    "    embeddings = []\n",
    "    for text_info in tqdm(data):\n",
    "        text = text_info['text']\n",
    "        result = text_info['result']\n",
    "        prompt = text\n",
    "        if save_kl: \n",
    "            kl, embedding = get_kl_and_embeddings([text], True)\n",
    "            if kl is not None :\n",
    "                kls.append(kl)\n",
    "                embeddings.append(embedding)\n",
    "        else :\n",
    "            embedding = get_kl_and_embeddings([text], False)\n",
    "            embeddings.append(embedding)\n",
    "            if kl is not None :\n",
    "                kls.append(kl)\n",
    "\n",
    "\n",
    "    # save kl divergence\n",
    "    if save_kl:\n",
    "        print(save_kl_dir+file_name+'.pkl')\n",
    "        pickle.dump(kls, open(save_kl_dir+file_name+'.pkl', 'wb'))\n",
    "\n",
    "    # save embeddings\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    print(save_embed_dir+file_name+'.pt')\n",
    "    torch.save(embeddings, save_embed_dir+file_name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_and_labels(file_name, data_path, device, layer_num):\n",
    "    print(\"trying to get\", file_name)\n",
    "    # print(data_path)\n",
    "    labels = torch.load(labels_folder + file_name + '.pt').to(device)\n",
    "    try:\n",
    "        embeddings = torch.load(save_embed_dir + file_name + '.pt')             \n",
    "    except FileNotFoundError:\n",
    "        # If saved embeddings not found, generate them\n",
    "        print('target file not found, start generating embeddings')\n",
    "        # print(data_path)\n",
    "        generate_embeddings(data_path, file_name, layer_num == -1) #only compute and save kl is layer_num==-1, which means max-kl\n",
    "        embeddings = torch.load(save_embed_dir + file_name + '.pt') \n",
    "    \n",
    "    if layer_num != -1:\n",
    "        embeddings = embeddings[:,embedding_dim * layer_num: embedding_dim * (layer_num + 1)].to(device)\n",
    "    else: #max_kl\n",
    "        # with open(f'save/kl_divergence/last_token/' + file_name + '.pkl', 'rb') as f:\n",
    "        with open(save_kl_dir + file_name + '.pkl', 'rb') as f:\n",
    "            kl = pickle.load(f)\n",
    "            kl = np.array(kl)\n",
    "            idx = kl.argmax(axis=1)\n",
    "            embeddings = torch.tensor([row[(i+1)*embedding_dim:(i+2)*embedding_dim].tolist() for row ,i in zip(embeddings,idx) ]).to(device)\n",
    "\n",
    "    return embeddings, labels\n",
    "    \n",
    "\n",
    "def get_train_eval_data(layer_num, device, train_num = 160, valid_num = 20) :\n",
    "    train_embeddings, train_labels = get_embeddings_and_labels(train_name, train_path, device, layer_num)\n",
    "    valid_embeddings, valid_labels = get_embeddings_and_labels(val_name, val_path, device, layer_num)\n",
    "    return train_embeddings[:train_num], train_labels[:train_num], valid_embeddings[:valid_num], valid_labels[:valid_num]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't edit this cell\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes=[1024, 512], num_labels=2, dropout_prob=0.2):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Dropout(dropout_prob),\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.Tanh(),\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        self.dense = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(prev_size, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(layer_num, device, hidden_sizes = [1024,512], droprate = 0.4, num_epochs = 10, learning_rate = 0.003):\n",
    "    train_embeddings, train_labels, valid_embeddings, valid_labels = get_train_eval_data(layer_num, device)\n",
    "    # print(train_embeddings.mean())\n",
    "    print(\"the device being used is\", device)\n",
    "    print(\"there are\", train_embeddings.shape[0], \"train data and\", valid_embeddings.shape[0], \"validation data.\")\n",
    "    input_size = train_embeddings.shape[1]\n",
    "    \n",
    "    model = BinaryClassifier(input_size,hidden_sizes=hidden_sizes,dropout_prob=droprate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    batch_size = 16\n",
    "    best_valid_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(0, len(train_embeddings), batch_size):\n",
    "            model.train()\n",
    "            batch_embeddings = train_embeddings[i:i+batch_size].to(device)\n",
    "            batch_labels = train_labels[i:i+batch_size].to(device)\n",
    "            outputs = model(batch_embeddings)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(valid_embeddings)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            accuracy = (predicted == valid_labels).sum().item() / len(valid_labels)\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(device, dataset_names, layer_num) :\n",
    "    testset_embeddings, testset_labels, data_and_model_names = [], [], []\n",
    "    for dataset_name in dataset_names: \n",
    "        if dataset == 'original':\n",
    "            for model_name in ['gpt3.5', 'gpt4', 'claude3']:\n",
    "                test_embeddings, test_labels = get_embeddings_and_labels(dataset_name + '_' + model_name, \n",
    "                    'processed_data/test_data/' + dataset_name + '_' + model_name + '.json', device, layer_num)\n",
    "                testset_embeddings.append(test_embeddings)\n",
    "                testset_labels.append(test_labels)\n",
    "                data_and_model_names.append(dataset_name + \"-\" + model_name)\n",
    "        else: \n",
    "            test_embeddings, test_labels = get_embeddings_and_labels('test', test_path, device, layer_num)\n",
    "            testset_embeddings.append(test_embeddings)\n",
    "            testset_labels.append(test_labels)\n",
    "            data_and_model_names.append(dataset_name)\n",
    "\n",
    "    return testset_embeddings, testset_labels, data_and_model_names\n",
    "\n",
    "def group_and_average(name_to_auroc):\n",
    "    grouped = defaultdict(list)\n",
    "    for name, auroc in name_to_auroc.items():\n",
    "        name_lower = name.lower()\n",
    "        if 'gpt3' in name_lower or 'chatgpt' in name_lower:\n",
    "            grouped['gpt3.5'].append(auroc)\n",
    "        elif 'gpt4' in name_lower:\n",
    "            grouped['gpt4'].append(auroc)\n",
    "        elif 'claude' in name_lower:\n",
    "            grouped['claude3'].append(auroc)\n",
    "        elif dataset == 'counseling':\n",
    "            grouped['counseling'].append(auroc)\n",
    "        else:\n",
    "            print(f\"Warning: could not classify {name}\")\n",
    "    \n",
    "    avg_aurocs = {}\n",
    "    for model_type in ['gpt3.5', 'gpt4', 'claude3', '', 'counseling']:\n",
    "        if grouped[model_type]:\n",
    "            avg_aurocs[model_type] = sum(grouped[model_type]) / len(grouped[model_type])\n",
    "        else:\n",
    "            avg_aurocs[model_type] = None\n",
    "    \n",
    "    return avg_aurocs\n",
    "\n",
    "\n",
    "def run_single_test(model,test_set,test_label,test_acc,testset_name):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_set)\n",
    "        probabilities = torch.softmax(outputs, dim=1)[:, 1]\n",
    "        auroc = roc_auc_score(test_label.cpu().numpy(), probabilities.cpu().numpy())\n",
    "        test_acc.append(auroc)\n",
    "    return auroc\n",
    "\n",
    "def run_all_tests(model, layer_num, device, dataset_names) :\n",
    "    testset_embeddings, testset_labels, data_and_model_names = get_test_data(device, dataset_names, layer_num)\n",
    "    with torch.no_grad():\n",
    "        name_to_auroc = {}\n",
    "        for test_embed, test_label, data_and_model_name in zip(testset_embeddings, testset_labels, data_and_model_names):\n",
    "            auroc = run_single_test(model, test_embed, test_label, [], ' ')\n",
    "            name_to_auroc[data_and_model_name] = auroc\n",
    "    return name_to_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_one_layer(layer_num, device, dataset_names):\n",
    "    model = train(layer_num, device)\n",
    "    name_to_auroc = run_all_tests(model, layer_num, device, dataset_names)\n",
    "    avg_aurocs = group_and_average(name_to_auroc)\n",
    "    return avg_aurocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get train\n",
      "trying to get val\n",
      "the device being used is cuda\n",
      "there are 160 train data and 20 validation data.\n",
      "Epoch [1/10], Loss: 0.4858, Validation Accuracy: 0.9500\n",
      "Epoch [2/10], Loss: 0.1949, Validation Accuracy: 0.9000\n",
      "Epoch [3/10], Loss: 0.0277, Validation Accuracy: 0.9000\n",
      "Epoch [4/10], Loss: 0.0330, Validation Accuracy: 1.0000\n",
      "Epoch [5/10], Loss: 0.0369, Validation Accuracy: 1.0000\n",
      "Epoch [6/10], Loss: 0.0048, Validation Accuracy: 1.0000\n",
      "Epoch [7/10], Loss: 0.0065, Validation Accuracy: 1.0000\n",
      "Epoch [8/10], Loss: 0.0821, Validation Accuracy: 0.9500\n",
      "Epoch [9/10], Loss: 0.0023, Validation Accuracy: 1.0000\n",
      "Epoch [10/10], Loss: 0.0019, Validation Accuracy: 1.0000\n",
      "trying to get test\n",
      "target file not found, start generating embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████▋| 199/200 [38:25<00:11, 11.80s/it]"
     ]
    }
   ],
   "source": [
    "train_and_test_one_layer(-1, device, dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get train\n",
      "trying to get val\n",
      "the device being used is cuda\n",
      "there are 144 train data and 16 validation data.\n",
      "Epoch [1/10], Loss: 0.9041, Validation Accuracy: 0.5000\n",
      "Epoch [2/10], Loss: 0.6389, Validation Accuracy: 0.5000\n",
      "Epoch [3/10], Loss: 0.6393, Validation Accuracy: 0.5000\n",
      "Epoch [4/10], Loss: 0.6127, Validation Accuracy: 0.5000\n",
      "Epoch [5/10], Loss: 0.6349, Validation Accuracy: 0.5000\n",
      "Epoch [6/10], Loss: 0.6226, Validation Accuracy: 0.5000\n",
      "Epoch [7/10], Loss: 0.6307, Validation Accuracy: 0.5000\n",
      "Epoch [8/10], Loss: 0.6255, Validation Accuracy: 0.5000\n",
      "Epoch [9/10], Loss: 0.6565, Validation Accuracy: 0.5000\n",
      "Epoch [10/10], Loss: 0.7095, Validation Accuracy: 0.5000\n",
      "trying to get test\n",
      "{'gpt3.5': None, 'gpt4': None, 'claude3': None, '': None, 'counseling': 0.5}\n",
      "trying to get train\n",
      "trying to get val\n",
      "the device being used is cuda\n",
      "there are 144 train data and 16 validation data.\n",
      "Epoch [1/10], Loss: 0.5637, Validation Accuracy: 0.5000\n",
      "Epoch [2/10], Loss: 0.4105, Validation Accuracy: 0.5000\n",
      "Epoch [3/10], Loss: 0.1521, Validation Accuracy: 1.0000\n",
      "Epoch [4/10], Loss: 0.0737, Validation Accuracy: 0.9375\n",
      "Epoch [5/10], Loss: 0.0556, Validation Accuracy: 1.0000\n",
      "Epoch [6/10], Loss: 0.0124, Validation Accuracy: 0.9375\n",
      "Epoch [7/10], Loss: 0.0088, Validation Accuracy: 0.9375\n",
      "Epoch [8/10], Loss: 0.0136, Validation Accuracy: 1.0000\n",
      "Epoch [9/10], Loss: 0.0106, Validation Accuracy: 1.0000\n",
      "Epoch [10/10], Loss: 0.0126, Validation Accuracy: 1.0000\n",
      "trying to get test\n",
      "{'gpt3.5': None, 'gpt4': None, 'claude3': None, '': None, 'counseling': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# first layer\n",
    "avg_aurocs = train_and_test_one_layer(0, device, dataset_names)\n",
    "print(avg_aurocs)\n",
    "\n",
    "# last layer\n",
    "avg_aurocs = train_and_test_one_layer(32, device, dataset_names)\n",
    "print(avg_aurocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_to_aurocs = {'gpt3.5': [], 'gpt4': [], 'claude3': []}\n",
    "# layer_nums = []\n",
    "\n",
    "# for layer_num in range(33):\n",
    "#     print(\"layer_num =\", layer_num)\n",
    "#     layer_nums.append(layer_num)\n",
    "#     avg_aurocs = train_and_test_one_layer(layer_num, device)\n",
    "#     for model_type in ['gpt3.5', 'gpt4', 'claude3']:\n",
    "#         layer_to_aurocs[model_type].append(avg_aurocs[model_type])\n",
    "\n",
    "\n",
    "# # plot (similar to Figure 2 in their paper)\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# for model_type, aucs in layer_to_aurocs.items():\n",
    "#     plt.plot(layer_nums, aucs, label=model_type)\n",
    "\n",
    "# plt.xlabel('Layer Number')\n",
    "# plt.ylabel('Average AUROC')\n",
    "# plt.title('Layer vs AUROC for Different Models')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get HC3_en_train\n",
      "trying to get HC3_en_valid\n",
      "the device being used is cuda\n",
      "there are 160 train data and 20 validation data.\n",
      "Epoch [1/10], Loss: 0.8320, Validation Accuracy: 0.5000\n",
      "Epoch [2/10], Loss: 0.6462, Validation Accuracy: 1.0000\n",
      "Epoch [3/10], Loss: 0.2196, Validation Accuracy: 1.0000\n",
      "Epoch [4/10], Loss: 0.0632, Validation Accuracy: 1.0000\n",
      "Epoch [5/10], Loss: 0.0240, Validation Accuracy: 1.0000\n",
      "Epoch [6/10], Loss: 0.0118, Validation Accuracy: 1.0000\n",
      "Epoch [7/10], Loss: 0.0133, Validation Accuracy: 1.0000\n",
      "Epoch [8/10], Loss: 0.0324, Validation Accuracy: 1.0000\n",
      "Epoch [9/10], Loss: 0.0344, Validation Accuracy: 1.0000\n",
      "Epoch [10/10], Loss: 0.0394, Validation Accuracy: 1.0000\n",
      "trying to get pub_gpt3.5\n",
      "trying to get pub_gpt4\n",
      "trying to get pub_claude3\n",
      "{'gpt3.5': 0.8608444444444445, 'gpt4': 0.8959956758704563, 'claude3': 0.985204081632653}\n"
     ]
    }
   ],
   "source": [
    "# # first layer\n",
    "# avg_aurocs = train_and_test_one_layer(0, device)\n",
    "# print(avg_aurocs)\n",
    "\n",
    "# last layer\n",
    "avg_aurocs = train_and_test_one_layer(22, device, ['pub'])\n",
    "print(avg_aurocs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupenv)",
   "language": "python",
   "name": "jupenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
