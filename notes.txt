import pickle
import numpy as np
with open('/homes/iws/ycui7/Text-Fluoroscopy/save/kl_divergence/last_token_2/pub_gpt3.5.pkl', 'rb') as f:
  data = pickle.load(f)

kl = np.array(data)
kl = np.array(data)
idx = kl.argmax(axis=1)
print(idx)


I tried use only the embedding of last token to calculate kl and printed out the chosen tokens for train data. The majority of them are very different from that chosen by all tokens:

pub_gpt3.5, selected layer: 
all_tokens: 
[30 30 30 30 30 29 30 30 29 30 29 29 30 30 29 29 30 30 30 29 30 30 30 30
 30 30 29 29 29 29 30 29 30 30 30 30 30 30 29 30 30 30 29 30 30 30 30 30
 30 30 30 30 30 30 30 29 30 30 30 30 30 30 29 29 30 30 30 30 30 30 29 30
 30 30 30 30 29 29 30 30 30 30 29 29 30 30 30 30 30 29 30 30 30 30 30 29
 30 30 30 30 30 30 30 30 30 30 29 30 30 30 29 29 29 29 30 30 29 29 29 29
 30 30 29 29 30 30 30 30 30 30 29 29 30 30 30 30 30 29 30 30 29 29 30 30
 30 30 30 30 30 30 30 29 29 29 30 30 30 30 29 30 29 29 30 30 30 30 30 30
 30 30 30 30 30 30 30 30 30 30 29 30 30 30 29 30 30 30 30 30 30 30 29 29
 30 30 30 30 30 30 30 30 29 29 30 30 29 29 29 30 30 30 30 30 30 30 30 30
 30 29 29 30 30 29 30 30 30 30 30 29 30 30 29 29 30 30 29 29 30 30 30 30
 30 30 30 30 29 30 30 30 29 29 30 30 30 30 29 29 30 30 30 30 29 29 30 30
 29 30 30 30 29 29 29 29 30 30 29 29 30 30 30 30 29 29 30 30 30 30 30 30
 30 30 30 29 30 30 29 30 30 30 30 30]

last_token:
[10 30 10 10 30 10 10 10 10 30 10 10 10 10 10 30 10 10 10 10 10 10 30 10
 10 10 10 10 30 10 10 30 10 10 30 30 10 10 10 10 10 10 30 30 10 10 10 10
 10 30 30 10 10 10 10 10 10 30 10 30 10 10 10 10 10 10 10 10 10 10 10 30
 10 10 10 10 10 10 30 10 30 10 10 10 10 10 10 10 10 10 30 10 10 10 10 30
 10 30 10 30 10 30 10 10 10 10 10 10 10 10 30 10 10 10 10 10 10 10 10 10
 10 10 10 30 10 10 10 30 10 30 30 30 10 10 10 30 10 10 10 30 30 10 10 10
 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 30 10 10 30 30 10 10 10
 30 10 10 10 10 10 10 10 10 30 10 30 10 30 10 10 10 10 10 30 10 10 10 10
 10 10 10 10 10 30 30 10 10 10 10 10 10 10 30 10 30 30 10 10 10 30 30 30
 10 30 10 10 10 30 10 10 10 10 10 10 10 30 10 30 10 10 10 10 10 10 30 30
 10 10 10 10 10 10 10 10 10 10 10 10 10 10 30 30 10 10 10 10 10 30 10 30
 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 30 30 10 10 30 10 10 10
 10 10 10 10 10 30 10 10 10 10 10 10]


I also tried calculate kl with respect to first encoding layer and last encoding layer,
the result is very similar
[10 30 10 10 30 10 10 10 10 30 10 10 10 10 10 30 10 10 10 10 10 10 30 10
 10 10 10 10 30 10 10 30 10 10 30 30 10 10 10 10 10 10 30 30 10 10 10 10
 10 30 30 10 10 10 10 10 10 30 10 30 10 10 10 10 10 10 10 10 10 10 10 30
 10 10 10 10 10 10 30 10 30 10 10 10 10 10 10 10 10 10 30 10 10 10 10 30
 10 30 10 30 10 30 10 10 10 10 10 10 10 10 30 10 10 10 10 10 10 10 10 10
 10 10 10 30 10 10 10 30 10 30 30 30 10 10 10 30 10 10 10 30 30 10 10 10
 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 30 10 10 30 30 10 10 10
 30 10 10 10 10 10 10 10 10 30 10 30 10 30 10 10 10 10 10 30 10 10 10 10
 10 10 10 10 10 30 30 10 10 10 10 10 10 10 30 10 30 30 10 10 10 30 30 10
 10 30 10 10 10 30 10 10 10 10 10 10 10 30 10 30 10 10 10 10 10 10 30 30
 10 10 10 10 10 10 10 10 10 10 10 10 10 10 30 30 10 10 10 10 10 30 10 30
 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 30 30 10 10 30 10 10 10
 10 10 10 10 10 30 10 10 10 10 10 10]


pub_gpt4:
all_tokens: 
[30 30 30 30 30 30 30 30 29 30 29 29 30 30 29 29 30 30 30 30 30 30 30 30
 30 30 29 29 29 29 30 30 30 30 30 30 30 30 29 30 30 30 29 30 30 30 30 30
 30 30 30 30 30 30 30 29 30 30 30 29 30 30 29 29 30 30 30 30 30 30 29 29
 30 30 30 29 29 30 30 30 30 30 29 29 30 29 30 30 30 30 30 30 30 29 30 29
 30 30 30 30 30 29 30 30 30 30 29 30 30 30 29 29 29 29 30 29 29 29 29 29
 30 29 29 29 30 30 30 30 30 30 29 29 30 30 30 29 30 29 30 30 29 30 30 30
 30 30 30 30 30 29 29 29 30 30 30 30 29 30 29 29 30 30 30 29 30 30 30 30
 30 30 30 30 30 30 30 29 29 30 30 30 29 30 30 30 30 30 30 30 29 29 30 30
 30 30 30 30 30 29 29 29 30 30 29 29 29 30 30 30 30 30 30 30 30 29 30 30
 30 30 30 29 30 30 30 30 30 29 30 30 29 29 30 30 29 29 30 30 30 30 30 30
 30 30 29 30 30 29 29 29 30 30 30 30 29 29 30 30 30 30 29 29 30 30 29 30
 30 30 29 29 29 29 30 30 29 29 30 30 30 30 29 30 30 30 30 30 30 30 30 30
 30 30 30 30 29 30 30 30 30 30]

last_token:
[10 30 10 10 10 10 10 10 10 10 10 10 30 10 10 10 10 10 10 10 10 30 10 10
 30 10 30 10 30 10 10 10 10 10 10 10 10 30 10 30 10 30 10 30 10 30 10 10
 10 30 30 30 10 10 10 10 10 10 10 10 30 10 30 10 30 10 10 10 10 10 10 30
 10 10 10 10 10 10 30 10 30 30 10 10 10 10 10 10 10 10 10 10 10 10 10 30
 10 30 10 30 10 30 10 10 30 10 30 10 10 10 30 10 10 10 10 10 30 30 10 10
 10 10 10 10 10 10 30 10 10 10 10 10 10 10 10 10 10 30 10 10 30 10 10 10
 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 30 30 30 10 30 10 10
 10 10 10 10 10 10 10 10 10 30 10 30 10 10 10 30 10 10 10 10 10 30 10 10
 10 10 30 10 30 10 10 10 10 10 10 30 10 10 30 10 30 30 10 10 10 30 10 10
 30 10 10 10 10 10 10 10 10 30 10 10 10 30 10 10 10 10 10 10 30 10 10 10
 10 30 10 10 10 10 10 10 10 10 10 10 30 10 30 10 10 30 10 10 10 10 10 10
 10 30 10 10 10 10 10 30 10 10 10 10 10 10 10 10 10 10 10 10 30 10 10 10
 10 10 10 10 10 10 10 10 10 10]



use first layer: {‘gpt3.5': 0.5, 'gpt4': 0.5, 'claude3': 0.5}. # we have talked about this, the classifier learns nothing
use last layer: {'gpt3.5': 0.8663259259259259, 'gpt4': 0.8586380506717247, 'claude3': 0.8876125472411186}
their method (use only the first 300 tokens):{‘gpt3.5': 0.9301481481481483, 'gpt4': 0.9348033004852968, 'claude3': 0.9821760393046107}
